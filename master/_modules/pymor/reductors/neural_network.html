
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>pymor.reductors.neural_network &#8212; pyMOR v2019.2rc0+1448.g0b7e1854 Manual</title>
    <link rel="stylesheet" href="../../../_static/pymor.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/jupyter-sphinx.css" />
    
    <script id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/language_data.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.18.0/dist/embed-amd.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 
  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="../../../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="nav-item nav-item-0"><a href="../../../index.html">pyMOR v2019.2rc0+1448.g0b7e1854 Manual</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="../../index.html" accesskey="U">Module code</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">pymor.reductors.neural_network</a></li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <h1>Source code for pymor.reductors.neural_network</h1><div class="highlight"><pre>
<span></span><span class="c1"># This file is part of the pyMOR project (http://www.pymor.org).</span>
<span class="c1"># Copyright 2013-2020 pyMOR developers and contributors. All rights reserved.</span>
<span class="c1"># License: BSD 2-Clause License (http://opensource.org/licenses/BSD-2-Clause)</span>

<span class="kn">from</span> <span class="nn">pymor.core.config</span> <span class="kn">import</span> <span class="n">config</span>


<span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">HAVE_TORCH</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">numbers</span> <span class="kn">import</span> <span class="n">Number</span>

    <span class="kn">import</span> <span class="nn">torch</span>
    <span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
    <span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>
    <span class="kn">import</span> <span class="nn">torch.utils</span> <span class="k">as</span> <span class="nn">utils</span>

    <span class="kn">from</span> <span class="nn">pymor.algorithms.pod</span> <span class="kn">import</span> <span class="n">pod</span>
    <span class="kn">from</span> <span class="nn">pymor.core.base</span> <span class="kn">import</span> <span class="n">BasicObject</span>
    <span class="kn">from</span> <span class="nn">pymor.core.exceptions</span> <span class="kn">import</span> <span class="n">NeuralNetworkTrainingFailed</span>
    <span class="kn">from</span> <span class="nn">pymor.models.neural_network</span> <span class="kn">import</span> <span class="n">FullyConnectedNN</span><span class="p">,</span> <span class="n">NeuralNetworkModel</span>


<div class="viewcode-block" id="NeuralNetworkReductor"><a class="viewcode-back" href="../../../generated/pymor.reductors.html#pymor.reductors.neural_network.NeuralNetworkReductor">[docs]</a>    <span class="k">class</span> <span class="nc">NeuralNetworkReductor</span><span class="p">(</span><span class="n">BasicObject</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Reduced Basis reductor relying on artificial neural networks.</span>

<span class="sd">        This is a reductor that constructs a reduced basis using proper</span>
<span class="sd">        orthogonal decomposition and trains a neural network that approximates</span>
<span class="sd">        the mapping from parameter space to coefficients of the full-order</span>
<span class="sd">        solution in the reduced basis.</span>
<span class="sd">        The approach is described in [HU18]_.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        fom</span>
<span class="sd">            The full-order |Model| to reduce.</span>
<span class="sd">        training_set</span>
<span class="sd">            Set of |parameter values| to use for POD and training of the</span>
<span class="sd">            neural network.</span>
<span class="sd">        validation_set</span>
<span class="sd">            Set of |parameter values| to use for validation in the training</span>
<span class="sd">            of the neural network.</span>
<span class="sd">        validation_ratio</span>
<span class="sd">            Fraction of the training set to use for validation in the training</span>
<span class="sd">            of the neural network (only used if no validation set is provided).</span>
<span class="sd">        basis_size</span>
<span class="sd">            Desired size of the reduced basis. If `None`, rtol, atol or l2_err must</span>
<span class="sd">            be provided.</span>
<span class="sd">        rtol</span>
<span class="sd">            Relative tolerance the basis should guarantee on the training set.</span>
<span class="sd">        atol</span>
<span class="sd">            Absolute tolerance the basis should guarantee on the training set.</span>
<span class="sd">        l2_err</span>
<span class="sd">            L2-approximation error the basis should not exceed on the training</span>
<span class="sd">            set.</span>
<span class="sd">        pod_params</span>
<span class="sd">            Dict of additional parameters for the POD-method.</span>
<span class="sd">        ann_mse</span>
<span class="sd">            If `&#39;like_basis&#39;`, the mean squared error of the neural network on</span>
<span class="sd">            the training set should not exceed the error of projecting onto the basis.</span>
<span class="sd">            If `None`, the neural network with smallest validation error is</span>
<span class="sd">            used to build the ROM.</span>
<span class="sd">            If a tolerance is prescribed, the mean squared error of the neural</span>
<span class="sd">            network on the training set should not exceed this threshold.</span>
<span class="sd">            Training is interrupted if a neural network that undercuts the</span>
<span class="sd">            error tolerance is found.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fom</span><span class="p">,</span> <span class="n">training_set</span><span class="p">,</span> <span class="n">validation_set</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">validation_ratio</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                     <span class="n">basis_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">rtol</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">atol</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">l2_err</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">pod_params</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                     <span class="n">ann_mse</span><span class="o">=</span><span class="s1">&#39;like_basis&#39;</span><span class="p">):</span>
            <span class="k">assert</span> <span class="mi">0</span> <span class="o">&lt;</span> <span class="n">validation_ratio</span> <span class="o">&lt;</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">validation_set</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">__auto_init</span><span class="p">(</span><span class="nb">locals</span><span class="p">())</span>

<div class="viewcode-block" id="NeuralNetworkReductor.reduce"><a class="viewcode-back" href="../../../generated/pymor.reductors.html#pymor.reductors.neural_network.NeuralNetworkReductor.reduce">[docs]</a>        <span class="k">def</span> <span class="nf">reduce</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_layers</span><span class="o">=</span><span class="s1">&#39;[(N+P)*3, (N+P)*3]&#39;</span><span class="p">,</span> <span class="n">activation_function</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">,</span>
                   <span class="n">optimizer</span><span class="o">=</span><span class="n">optim</span><span class="o">.</span><span class="n">LBFGS</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span>
                   <span class="n">restarts</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
            <span class="sd">&quot;&quot;&quot;Reduce by training artificial neural networks.</span>

<span class="sd">            Parameters</span>
<span class="sd">            ----------</span>
<span class="sd">            hidden_layers</span>
<span class="sd">                Number of neurons in the hidden layers. Can either be fixed or</span>
<span class="sd">                a Python expression string depending on the reduced basis size</span>
<span class="sd">                `N` and the total dimension of the |Parameters| `P`.</span>
<span class="sd">            activation_function</span>
<span class="sd">                Activation function to use between the hidden layers.</span>
<span class="sd">            optimizer</span>
<span class="sd">                Algorithm to use as optimizer during training.</span>
<span class="sd">            epochs</span>
<span class="sd">                Maximum number of epochs for training.</span>
<span class="sd">            batch_size</span>
<span class="sd">                Batch size to use if optimizer allows mini-batching.</span>
<span class="sd">            learning_rate</span>
<span class="sd">                Step size to use in each optimization step.</span>
<span class="sd">            restarts</span>
<span class="sd">                Number of restarts of the training algorithm. Since the training</span>
<span class="sd">                results highly depend on the initial starting point, i.e. the</span>
<span class="sd">                initial weights and biases, it is advisable to train multiple</span>
<span class="sd">                neural networks by starting with different initial values and</span>
<span class="sd">                choose that one performing best on the validation set.</span>
<span class="sd">            seed</span>
<span class="sd">                Seed to use for various functions in PyTorch. Using a fixed seed,</span>
<span class="sd">                it is possible to reproduce former results.</span>

<span class="sd">            Returns</span>
<span class="sd">            -------</span>
<span class="sd">            rom</span>
<span class="sd">                Reduced-order |NeuralNetworkModel|.</span>
<span class="sd">            &quot;&quot;&quot;</span>
            <span class="k">assert</span> <span class="n">restarts</span> <span class="o">&gt;</span> <span class="mi">0</span>
            <span class="k">assert</span> <span class="n">epochs</span> <span class="o">&gt;</span> <span class="mi">0</span>
            <span class="k">assert</span> <span class="n">batch_size</span> <span class="o">&gt;</span> <span class="mi">0</span>
            <span class="k">assert</span> <span class="n">learning_rate</span> <span class="o">&gt;</span> <span class="mf">0.</span>

            <span class="c1"># set a seed for the PyTorch initialization of weights and biases and further PyTorch methods</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

            <span class="c1"># build a reduced basis using POD and compute training data</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;reduced_basis&#39;</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">reduced_basis</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">mse_basis</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">build_basis</span><span class="p">()</span>

            <span class="c1"># determine the numbers of neurons in the hidden layers</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">hidden_layers</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
                <span class="n">hidden_layers</span> <span class="o">=</span> <span class="nb">eval</span><span class="p">(</span><span class="n">hidden_layers</span><span class="p">,</span> <span class="p">{</span><span class="s1">&#39;N&#39;</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">reduced_basis</span><span class="p">),</span> <span class="s1">&#39;P&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">fom</span><span class="o">.</span><span class="n">parameters</span><span class="o">.</span><span class="n">dim</span><span class="p">})</span>
            <span class="c1"># input and output size of the neural network are prescribed by the dimension of the parameter space</span>
            <span class="c1"># and the reduced basis size</span>
            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">hidden_layers</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span>
            <span class="n">layers</span> <span class="o">=</span> <span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fom</span><span class="o">.</span><span class="n">parameters</span><span class="p">),]</span> <span class="o">+</span> <span class="n">hidden_layers</span> <span class="o">+</span> <span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">reduced_basis</span><span class="p">),]</span>

            <span class="c1"># compute validation data</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;validation_data&#39;</span><span class="p">):</span>
                <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s1">&#39;Computing validation snapshots ...&#39;</span><span class="p">):</span>

                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">validation_set</span><span class="p">:</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">validation_data</span> <span class="o">=</span> <span class="p">[]</span>
                        <span class="k">for</span> <span class="n">mu</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">validation_set</span><span class="p">:</span>
                            <span class="n">mu_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">DoubleTensor</span><span class="p">(</span><span class="n">mu</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">())</span>
                            <span class="n">u</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fom</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">mu</span><span class="p">)</span>
                            <span class="n">u_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">DoubleTensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">reduced_basis</span><span class="o">.</span><span class="n">inner</span><span class="p">(</span><span class="n">u</span><span class="p">)[:,</span><span class="mi">0</span><span class="p">])</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">validation_data</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">mu_tensor</span><span class="p">,</span> <span class="n">u_tensor</span><span class="p">))</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">number_validation_snapshots</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">training_data</span><span class="p">)</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">validation_ratio</span><span class="p">)</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">validation_data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_data</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">number_validation_snapshots</span><span class="p">]</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">training_data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_data</span><span class="p">[</span><span class="n">number_validation_snapshots</span><span class="o">+</span><span class="mi">1</span><span class="p">:]</span>

            <span class="c1"># run the actual training of the neural network</span>
            <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Performing </span><span class="si">{</span><span class="n">restarts</span><span class="si">}</span><span class="s1"> restarts for training ...&#39;</span><span class="p">):</span>

                <span class="k">for</span> <span class="n">run</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">restarts</span><span class="p">):</span>
                    <span class="n">neural_network</span><span class="p">,</span> <span class="n">current_losses</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_train</span><span class="p">(</span><span class="n">layers</span><span class="p">,</span> <span class="n">activation_function</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span>
                                                       <span class="n">epochs</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">)</span>
                    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;losses&#39;</span><span class="p">)</span> <span class="ow">or</span> <span class="n">current_losses</span><span class="p">[</span><span class="s1">&#39;val&#39;</span><span class="p">]</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">losses</span><span class="p">[</span><span class="s1">&#39;val&#39;</span><span class="p">]:</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">losses</span> <span class="o">=</span> <span class="n">current_losses</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">neural_network</span> <span class="o">=</span> <span class="n">neural_network</span>

                        <span class="c1"># check if neural network is sufficient to guarantee certain error bounds</span>
                        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s1">&#39;Checking tolerances for error of neural network ...&#39;</span><span class="p">):</span>

                            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ann_mse</span><span class="p">,</span> <span class="n">Number</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">losses</span><span class="p">[</span><span class="s1">&#39;full&#39;</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ann_mse</span><span class="p">:</span>
                                <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Aborting training after </span><span class="si">{</span><span class="n">run</span><span class="si">}</span><span class="s1"> restarts ...&#39;</span><span class="p">)</span>
                                <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_build_rom</span><span class="p">()</span>
                            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">ann_mse</span> <span class="o">==</span> <span class="s1">&#39;like_basis&#39;</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">losses</span><span class="p">[</span><span class="s1">&#39;full&#39;</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mse_basis</span><span class="p">:</span>
                                <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Aborting training after </span><span class="si">{</span><span class="n">run</span><span class="si">}</span><span class="s1"> restarts ...&#39;</span><span class="p">)</span>
                                <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_build_rom</span><span class="p">()</span>


            <span class="c1"># check if neural network is sufficient to guarantee certain error bounds</span>
            <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s1">&#39;Checking tolerances for error of neural network ...&#39;</span><span class="p">):</span>

                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ann_mse</span><span class="p">,</span> <span class="n">Number</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">losses</span><span class="p">[</span><span class="s1">&#39;full&#39;</span><span class="p">]</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">ann_mse</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="n">NeuralNetworkTrainingFailed</span><span class="p">(</span><span class="s1">&#39;Could not train a neural network that &#39;</span>
                                                      <span class="s1">&#39;guarantees prescribed tolerance!&#39;</span><span class="p">)</span>
                <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">ann_mse</span> <span class="o">==</span> <span class="s1">&#39;like_basis&#39;</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">losses</span><span class="p">[</span><span class="s1">&#39;full&#39;</span><span class="p">]</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">mse_basis</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="n">NeuralNetworkTrainingFailed</span><span class="p">(</span><span class="s1">&#39;Could not train a neural network with an error as small as the &#39;</span>
                                                      <span class="s1">&#39;reduced basis error! Maybe you can try a different neural &#39;</span>
                                                      <span class="s1">&#39;network architecture or change the value of `ann_mse`.&#39;</span><span class="p">)</span>
                <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">ann_mse</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;Using neural network with smallest validation error ...&#39;</span><span class="p">)</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Finished training with a validation loss of </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">losses</span><span class="p">[</span><span class="s2">&quot;val&quot;</span><span class="p">]</span><span class="si">}</span><span class="s1"> ...&#39;</span><span class="p">)</span>
                    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_build_rom</span><span class="p">()</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Unknown value for mean squared error of neural network&#39;</span><span class="p">)</span></div>


        <span class="k">def</span> <span class="nf">_build_rom</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="sd">&quot;&quot;&quot;Construct the reduced order model.&quot;&quot;&quot;</span>
            <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s1">&#39;Building ROM ...&#39;</span><span class="p">):</span>
                <span class="n">rom</span> <span class="o">=</span> <span class="n">NeuralNetworkModel</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">neural_network</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">fom</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s1">_reduced&#39;</span><span class="p">)</span>

            <span class="k">return</span> <span class="n">rom</span>

        <span class="k">def</span> <span class="nf">_train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">layers</span><span class="p">,</span> <span class="n">activation_function</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">epochs</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">):</span>
            <span class="sd">&quot;&quot;&quot;Perform a single training iteration and return the resulting neural network.&quot;&quot;&quot;</span>
            <span class="k">assert</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;training_data&#39;</span><span class="p">)</span>
            <span class="k">assert</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;validation_data&#39;</span><span class="p">)</span>

            <span class="c1"># LBFGS-optimizer does not support mini-batching, so the batch size needs to be adjusted</span>
            <span class="k">if</span> <span class="n">optimizer</span> <span class="o">==</span> <span class="n">optim</span><span class="o">.</span><span class="n">LBFGS</span><span class="p">:</span>
                <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">training_data</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">validation_data</span><span class="p">))</span>

            <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s1">&#39;Training the neural network ...&#39;</span><span class="p">):</span>

                <span class="c1"># initialize the neural network</span>
                <span class="n">neural_network</span> <span class="o">=</span> <span class="n">FullyConnectedNN</span><span class="p">(</span><span class="n">layers</span><span class="p">,</span>
                                                  <span class="n">activation_function</span><span class="o">=</span><span class="n">activation_function</span><span class="p">)</span><span class="o">.</span><span class="n">double</span><span class="p">()</span>

                <span class="c1"># initialize the optimizer</span>
                <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span><span class="p">(</span><span class="n">neural_network</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span>
                                      <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>

                <span class="n">loss_function</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
                <span class="n">early_stopping_scheduler</span> <span class="o">=</span> <span class="n">EarlyStoppingScheduler</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">training_data</span><span class="p">)</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">validation_data</span><span class="p">))</span>

                <span class="c1"># create the training and validation sets as well as the respective data loaders</span>
                <span class="n">training_dataset</span> <span class="o">=</span> <span class="n">CustomDataset</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">training_data</span><span class="p">)</span>
                <span class="n">validation_dataset</span> <span class="o">=</span> <span class="n">CustomDataset</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">validation_data</span><span class="p">)</span>
                <span class="n">phases</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">,</span> <span class="s1">&#39;val&#39;</span><span class="p">]</span>
                <span class="n">training_loader</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">training_dataset</span><span class="p">,</span>
                                                        <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>
                <span class="n">validation_loader</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">validation_dataset</span><span class="p">,</span>
                                                          <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>
                <span class="n">dataloaders</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;train&#39;</span><span class="p">:</span>  <span class="n">training_loader</span><span class="p">,</span> <span class="s1">&#39;val&#39;</span><span class="p">:</span> <span class="n">validation_loader</span><span class="p">}</span>

                <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;Starting optimization procedure ...&#39;</span><span class="p">)</span>

                <span class="c1"># perform optimization procedure</span>
                <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
                    <span class="n">losses</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;full&#39;</span><span class="p">:</span> <span class="mf">0.</span><span class="p">}</span>

                    <span class="c1"># alternate between training and validation phase</span>
                    <span class="k">for</span> <span class="n">phase</span> <span class="ow">in</span> <span class="n">phases</span><span class="p">:</span>
                        <span class="k">if</span> <span class="n">phase</span> <span class="o">==</span> <span class="s1">&#39;train&#39;</span><span class="p">:</span>
                            <span class="n">neural_network</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
                        <span class="k">else</span><span class="p">:</span>
                            <span class="n">neural_network</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

                        <span class="n">running_loss</span> <span class="o">=</span> <span class="mf">0.0</span>

                        <span class="c1"># iterate over batches</span>
                        <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">dataloaders</span><span class="p">[</span><span class="n">phase</span><span class="p">]:</span>
                            <span class="n">inputs</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
                            <span class="n">targets</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

                            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="n">phase</span> <span class="o">==</span> <span class="s1">&#39;train&#39;</span><span class="p">):</span>
                                <span class="k">def</span> <span class="nf">closure</span><span class="p">():</span>
                                    <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_grad_enabled</span><span class="p">():</span>
                                        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
                                    <span class="n">outputs</span> <span class="o">=</span> <span class="n">neural_network</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
                                    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_function</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
                                    <span class="k">if</span> <span class="n">loss</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
                                        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
                                    <span class="k">return</span> <span class="n">loss</span>

                                <span class="c1"># perform optimization step</span>
                                <span class="k">if</span> <span class="n">phase</span> <span class="o">==</span> <span class="s1">&#39;train&#39;</span><span class="p">:</span>
                                    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">closure</span><span class="p">)</span>

                                <span class="c1"># compute loss of current batch</span>
                                <span class="n">loss</span> <span class="o">=</span> <span class="n">closure</span><span class="p">()</span>

                            <span class="c1"># update overall absolute loss</span>
                            <span class="n">running_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

                        <span class="c1"># compute average loss</span>
                        <span class="n">epoch_loss</span> <span class="o">=</span> <span class="n">running_loss</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataloaders</span><span class="p">[</span><span class="n">phase</span><span class="p">]</span><span class="o">.</span><span class="n">dataset</span><span class="p">)</span>

                        <span class="n">losses</span><span class="p">[</span><span class="n">phase</span><span class="p">]</span> <span class="o">=</span> <span class="n">epoch_loss</span>

                        <span class="n">losses</span><span class="p">[</span><span class="s1">&#39;full&#39;</span><span class="p">]</span> <span class="o">+=</span> <span class="n">running_loss</span>

                        <span class="c1"># check for early stopping</span>
                        <span class="k">if</span> <span class="n">phase</span> <span class="o">==</span> <span class="s1">&#39;val&#39;</span> <span class="ow">and</span> <span class="n">early_stopping_scheduler</span><span class="p">(</span><span class="n">losses</span><span class="p">,</span> <span class="n">neural_network</span><span class="p">):</span>
                            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">logging_disabled</span><span class="p">:</span>
                                <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Early stopping training process after </span><span class="si">{</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s1"> epochs ...&#39;</span><span class="p">)</span>
                                <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;Minimum validation loss: &#39;</span>
                                                 <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">early_stopping_scheduler</span><span class="o">.</span><span class="n">best_losses</span><span class="p">[</span><span class="s2">&quot;val&quot;</span><span class="p">]</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
                            <span class="k">return</span> <span class="n">early_stopping_scheduler</span><span class="o">.</span><span class="n">best_neural_network</span><span class="p">,</span> <span class="n">early_stopping_scheduler</span><span class="o">.</span><span class="n">best_losses</span>

            <span class="k">return</span> <span class="n">early_stopping_scheduler</span><span class="o">.</span><span class="n">best_neural_network</span><span class="p">,</span> <span class="n">early_stopping_scheduler</span><span class="o">.</span><span class="n">best_losses</span>

<div class="viewcode-block" id="NeuralNetworkReductor.build_basis"><a class="viewcode-back" href="../../../generated/pymor.reductors.html#pymor.reductors.neural_network.NeuralNetworkReductor.build_basis">[docs]</a>        <span class="k">def</span> <span class="nf">build_basis</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="sd">&quot;&quot;&quot;Compute a reduced basis using proper orthogonal decomposition.&quot;&quot;&quot;</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">training_data</span> <span class="o">=</span> <span class="p">[]</span>

            <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s1">&#39;Building reduced basis ...&#39;</span><span class="p">):</span>

                <span class="c1"># compute snapshots for POD and training of neural networks</span>
                <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s1">&#39;Computing training snapshots ...&#39;</span><span class="p">):</span>
                    <span class="n">U</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fom</span><span class="o">.</span><span class="n">solution_space</span><span class="o">.</span><span class="n">empty</span><span class="p">()</span>
                    <span class="k">for</span> <span class="n">mu</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_set</span><span class="p">:</span>
                        <span class="n">U</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fom</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">mu</span><span class="p">))</span>

                <span class="c1"># compute reduced basis via POD</span>
                <span class="n">reduced_basis</span><span class="p">,</span> <span class="n">svals</span> <span class="o">=</span> <span class="n">pod</span><span class="p">(</span><span class="n">U</span><span class="p">,</span> <span class="n">modes</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">basis_size</span><span class="p">,</span> <span class="n">rtol</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">rtol</span> <span class="o">/</span> <span class="mf">2.</span><span class="p">,</span>
                                           <span class="n">atol</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">atol</span> <span class="o">/</span> <span class="mf">2.</span><span class="p">,</span> <span class="n">l2_err</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">l2_err</span> <span class="o">/</span> <span class="mf">2.</span><span class="p">,</span>
                                           <span class="o">**</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pod_params</span> <span class="ow">or</span> <span class="p">{}))</span>

                <span class="c1"># determine the coefficients of the full-order solutions in the reduced basis to obtain the</span>
                <span class="c1"># training data; convert everything into tensors that are compatible with PyTorch</span>
                <span class="k">for</span> <span class="n">mu</span><span class="p">,</span> <span class="n">u</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">training_set</span><span class="p">,</span> <span class="n">U</span><span class="p">):</span>
                    <span class="n">mu_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">DoubleTensor</span><span class="p">(</span><span class="n">mu</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">())</span>
                    <span class="n">u_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">DoubleTensor</span><span class="p">(</span><span class="n">reduced_basis</span><span class="o">.</span><span class="n">inner</span><span class="p">(</span><span class="n">u</span><span class="p">)[:,</span><span class="mi">0</span><span class="p">])</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">training_data</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">mu_tensor</span><span class="p">,</span> <span class="n">u_tensor</span><span class="p">))</span>

            <span class="c1"># compute mean square loss</span>
            <span class="n">mean_square_loss</span> <span class="o">=</span> <span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">U</span><span class="o">.</span><span class="n">norm2</span><span class="p">())</span> <span class="o">-</span> <span class="nb">sum</span><span class="p">(</span><span class="n">svals</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">U</span><span class="p">)</span>

            <span class="k">return</span> <span class="n">reduced_basis</span><span class="p">,</span> <span class="n">mean_square_loss</span></div>

<div class="viewcode-block" id="NeuralNetworkReductor.reconstruct"><a class="viewcode-back" href="../../../generated/pymor.reductors.html#pymor.reductors.neural_network.NeuralNetworkReductor.reconstruct">[docs]</a>        <span class="k">def</span> <span class="nf">reconstruct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">u</span><span class="p">):</span>
            <span class="sd">&quot;&quot;&quot;Reconstruct high-dimensional vector from reduced vector `u`.&quot;&quot;&quot;</span>
            <span class="k">assert</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;reduced_basis&#39;</span><span class="p">)</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduced_basis</span><span class="o">.</span><span class="n">lincomb</span><span class="p">(</span><span class="n">u</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">())</span></div></div>


<div class="viewcode-block" id="EarlyStoppingScheduler"><a class="viewcode-back" href="../../../generated/pymor.reductors.html#pymor.reductors.neural_network.EarlyStoppingScheduler">[docs]</a>    <span class="k">class</span> <span class="nc">EarlyStoppingScheduler</span><span class="p">(</span><span class="n">BasicObject</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Class for performing early stopping in training of neural networks.</span>

<span class="sd">        If the validation loss does not decrease over a certain amount of epochs, the</span>
<span class="sd">        training should be aborted to avoid overfitting the training data.</span>
<span class="sd">        This class implements an early stopping scheduler that recommends to stop the</span>
<span class="sd">        training process if the validation loss did not decrease by at least `delta`</span>
<span class="sd">        over `patience` epochs.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        size_training_validation_set</span>
<span class="sd">            Size of both, training and validation set together.</span>
<span class="sd">        patience</span>
<span class="sd">            Number of epochs of non-decreasing validation loss allowed, before early</span>
<span class="sd">            stopping the training process.</span>
<span class="sd">        delta</span>
<span class="sd">            Minimal amount of decrease in the validation loss that is required to reset</span>
<span class="sd">            the counter of non-decreasing epochs.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size_training_validation_set</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">delta</span><span class="o">=</span><span class="mf">0.</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">__auto_init</span><span class="p">(</span><span class="nb">locals</span><span class="p">())</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">best_losses</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">best_neural_network</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">counter</span> <span class="o">=</span> <span class="mi">0</span>

<div class="viewcode-block" id="EarlyStoppingScheduler.__call__"><a class="viewcode-back" href="../../../generated/pymor.reductors.html#pymor.reductors.neural_network.EarlyStoppingScheduler.__call__">[docs]</a>        <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">losses</span><span class="p">,</span> <span class="n">neural_network</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
            <span class="sd">&quot;&quot;&quot;Returns `True` if early stopping of training is suggested.</span>

<span class="sd">            Parameters</span>
<span class="sd">            ----------</span>
<span class="sd">            losses</span>
<span class="sd">                Dictionary of losses on the validation and the training set in</span>
<span class="sd">                the current epoch.</span>
<span class="sd">            neural_network</span>
<span class="sd">                Neural network that produces the current validation loss.</span>

<span class="sd">            Returns</span>
<span class="sd">            -------</span>
<span class="sd">            `True` if early stopping is suggested, `False` otherwise.</span>
<span class="sd">            &quot;&quot;&quot;</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">best_losses</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">best_losses</span> <span class="o">=</span> <span class="n">losses</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">best_losses</span><span class="p">[</span><span class="s1">&#39;full&#39;</span><span class="p">]</span> <span class="o">/=</span> <span class="bp">self</span><span class="o">.</span><span class="n">size_training_validation_set</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">best_neural_network</span> <span class="o">=</span> <span class="n">neural_network</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">best_losses</span><span class="p">[</span><span class="s1">&#39;val&#39;</span><span class="p">]</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">delta</span> <span class="o">&lt;=</span> <span class="n">losses</span><span class="p">[</span><span class="s1">&#39;val&#39;</span><span class="p">]:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">counter</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">counter</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">patience</span><span class="p">:</span>
                    <span class="k">return</span> <span class="kc">True</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">best_losses</span> <span class="o">=</span> <span class="n">losses</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">best_losses</span><span class="p">[</span><span class="s1">&#39;full&#39;</span><span class="p">]</span> <span class="o">/=</span> <span class="bp">self</span><span class="o">.</span><span class="n">size_training_validation_set</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">best_neural_network</span> <span class="o">=</span> <span class="n">neural_network</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">counter</span> <span class="o">=</span> <span class="mi">0</span>

            <span class="k">return</span> <span class="kc">False</span></div></div>


<div class="viewcode-block" id="CustomDataset"><a class="viewcode-back" href="../../../generated/pymor.reductors.html#pymor.reductors.neural_network.CustomDataset">[docs]</a>    <span class="k">class</span> <span class="nc">CustomDataset</span><span class="p">(</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Class that represents the dataset to use in PyTorch.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        training_data</span>
<span class="sd">            Set of training parameters and the respective coefficients of the</span>
<span class="sd">            solution in the reduced basis.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">training_data</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">training_data</span> <span class="o">=</span> <span class="n">training_data</span>

        <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">training_data</span><span class="p">)</span>

        <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
            <span class="n">t</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_data</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
            <span class="k">return</span> <span class="n">t</span></div>
</pre></div>

            <div class="clearer"></div>
          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="../../../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="nav-item nav-item-0"><a href="../../../index.html">pyMOR v2019.2rc0+1448.g0b7e1854 Manual</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="../../index.html" >Module code</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">pymor.reductors.neural_network</a></li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 2013-2020 pyMOR developers and contributors.
      Last updated on Jul 20, 2020.
      Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 3.1.2.
    </div>
  </body>
</html>